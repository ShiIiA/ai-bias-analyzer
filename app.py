# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rRSegdTULlczjwu11IQzYDFAIJmlE8TF
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import openai
import shap
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from fairlearn.reductions import ExponentiatedGradient, DemographicParity
from fairlearn.metrics import demographic_parity_difference
from PIL import Image
import matplotlib.pyplot as plt
import seaborn as sns

# === SETUP CONFIGURATION ===
st.set_page_config(page_title="AI Bias Analyzer", layout="wide")

# === LOAD PRETRAINED MODEL ===
MODEL_NAME = "microsoft/deberta-v3-base"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)

# === SHAP EXPLAINER SETUP ===
masker = shap.maskers.Text(tokenizer)
explainer = shap.Explainer(model, masker=masker)

# === LOAD DATA ===
uploaded_file = st.file_uploader("Upload a dataset", type=["csv"])
if uploaded_file:
    df = pd.read_csv(uploaded_file)
    st.write("üìä Preview of your dataset:")
    st.dataframe(df.head())

# === MODEL BIAS ANALYSIS ===
st.header("üîç Bias Analysis")
target_column = st.selectbox("Select target variable:", df.columns if uploaded_file else [])

if uploaded_file and target_column:
    # Compute Fairness Metrics
    sensitive_attr = st.selectbox("Select sensitive attribute:", df.columns)

    demographic_parity = demographic_parity_difference(
        df[target_column], df[sensitive_attr]
    )

    st.write(f"üì¢ **Demographic Parity Difference:** {demographic_parity:.4f}")

    # SHAP Interpretation
    st.subheader("Explainability using SHAP")
    sample_text = st.text_area("Enter a sample text for analysis:")

    if sample_text:
        tokens = tokenizer(sample_text, return_tensors="pt")
        shap_values = explainer(tokens)

        st.write("üìä SHAP Explanation for Prediction:")
        shap.plots.text(shap_values)

# === BIAS REDUCTION METHODS ===
st.header("‚öñÔ∏è Bias Reduction Techniques")

bias_methods = {
    "Reweighting": "Adjust sample weights to ensure fairness.",
    "Adversarial Debiasing": "Train an adversarial model to reduce bias.",
    "Post-Processing": "Modify predictions to achieve fairness.",
}

selected_method = st.selectbox("Choose a bias reduction technique:", list(bias_methods.keys()))

st.write(f"üí° **Explanation:** {bias_methods[selected_method]}")

# === UX DESIGN EXPLANATION ===
st.header("üé® UX Design & Model Transparency")

st.write(
    """
    - **Transparency**: This app highlights fairness metrics and model explanations.
    - **User Control**: Select bias reduction techniques and interpret model outputs.
    - **Visualizations**: Graphs and text-based explanations aid decision-making.
    """
)

# === FINAL MESSAGE ===
st.success("üéâ AI Bias Analyzer Ready!")