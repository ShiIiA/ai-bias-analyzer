# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rRSegdTULlczjwu11IQzYDFAIJmlE8TF
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import openai
import torch
import shap
import matplotlib.pyplot as plt
import seaborn as sns

# === AI & NLP Imports ===
from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline
from PIL import Image

# === Fairness and Bias Analysis ===
from fairlearn.reductions import ExponentiatedGradient, DemographicParity
from fairlearn.metrics import demographic_parity_difference, equalized_odds_difference

# === CONFIGURATION ===
st.set_page_config(page_title="AI Bias Analyzer", layout="wide")

# === FIX DEBERTA TOKENIZER ===
try:
    tokenizer = AutoTokenizer.from_pretrained(
        "microsoft/deberta-v3-base",
        use_fast=False,  # Ensure slow tokenizer is used
        trust_remote_code=True
    )
except ValueError:
    st.error("Error loading DeBERTa tokenizer. Ensure `sentencepiece` is installed.")

model = AutoModelForSequenceClassification.from_pretrained("microsoft/deberta-v3-base")

# === SHAP EXPLAINER FIX: USING A TEXT MASKER ===
masker = shap.maskers.Text(tokenizer)
explainer = shap.Explainer(model, masker=masker)

# === LOAD DATA ===
@st.cache_data
def load_data():
    return pd.read_csv("sample_data.csv")  # Ensure dataset is available

df = load_data()

# === UI DESIGN ===
st.title("üîç AI Bias Analyzer - Snakelets Team üêç")
st.markdown("""
**Analyze model bias, fairness, and interpretability.**
""")

# === DATA UPLOAD ===
st.sidebar.header("Upload Dataset")
uploaded_file = st.sidebar.file_uploader("Choose a CSV file", type="csv")

if uploaded_file:
    df = pd.read_csv(uploaded_file)
    st.success("‚úÖ File uploaded successfully!")

# === MODEL PREDICTION ===
st.header("üìä Model Prediction Analysis")
selected_feature = st.selectbox("Choose feature to analyze", df.columns)

if st.button("Run Model"):
    st.write("Generating Predictions...")

    # Tokenization
    inputs = tokenizer(df[selected_feature].astype(str).tolist(), truncation=True, padding=True, return_tensors="pt")
    with torch.no_grad():
        outputs = model(**inputs)

    predictions = torch.softmax(outputs.logits, dim=1).numpy()
    df["Prediction"] = predictions[:, 1]  # Assuming binary classification

    st.dataframe(df)

# === EXPLAINABILITY & SHAP ANALYSIS ===
st.header("üßê Understanding Model Decisions")

if st.button("Run SHAP Analysis"):
    st.write("Computing SHAP values...")

    shap_values = explainer(df[selected_feature].astype(str).tolist())

    fig, ax = plt.subplots(figsize=(10, 5))
    shap.summary_plot(shap_values, df[selected_feature], plot_type="bar", show=False)
    st.pyplot(fig)

# === BIAS ANALYSIS ===
st.header("‚öñÔ∏è Bias Analysis")

# Select sensitive attribute
sensitive_feature = st.selectbox("Select a sensitive attribute", df.columns)

# Compute bias metrics
dp_diff = demographic_parity_difference(df["Prediction"], df[sensitive_feature])
eo_diff = equalized_odds_difference(df["Prediction"], df[sensitive_feature])

st.write(f"**Demographic Parity Difference:** {dp_diff:.4f}")
st.write(f"**Equalized Odds Difference:** {eo_diff:.4f}")

# === VISUALIZING BIAS ===
fig = px.histogram(df, x="Prediction", color=sensitive_feature, title="Prediction Distribution by Sensitive Attribute")
st.plotly_chart(fig)

# === BIAS REDUCTION METHODS ===
st.header("üõ† Bias Mitigation Strategies")

bias_reduction_method = st.selectbox("Choose bias mitigation method", ["None", "Reweighting", "Exponentiated Gradient"])

if bias_reduction_method == "Reweighting":
    st.write("üîπ **Reweighting Strategy**: Adjusts sample weights to balance fairness trade-offs.")
    # Implement weight adjustment
elif bias_reduction_method == "Exponentiated Gradient":
    st.write("üîπ **Exponentiated Gradient**: Constrains model to satisfy fairness constraints.")
    mitigator = ExponentiatedGradient(model, constraints=DemographicParity())
    mitigator.fit(df[selected_feature], df["Prediction"])
    df["Mitigated Prediction"] = mitigator.predict(df[selected_feature])
    st.dataframe(df[["Prediction", "Mitigated Prediction"]])

# === UX DESIGN & EXPLANATIONS ===
st.header("üé® UX Design Considerations")
st.markdown("""
- **Clarity**: Results are visualized using plots for better interpretation.
- **Interactivity**: Users can explore different bias mitigation methods.
- **Transparency**: SHAP analysis helps users understand how models make decisions.
""")

st.header("ü§ñ AI Agent Explanation")
if st.button("Ask AI for Explanation"):
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are an AI assistant helping analyze bias in AI models."},
            {"role": "user", "content": f"Explain the bias analysis results: DP={dp_diff:.4f}, EO={eo_diff:.4f}"}
        ]
    )
    st.write(response["choices"][0]["message"]["content"])

st.success("üéâ Analysis Complete!")

# === FOOTER ===
st.markdown(
    "<br><hr><p style='text-align:center;'>üêç Snakelets | AI Bias Analyzer | Made with ‚ù§Ô∏è</p>",
    unsafe_allow_html=True
)