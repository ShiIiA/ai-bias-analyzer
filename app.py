# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rRSegdTULlczjwu11IQzYDFAIJmlE8TF
"""

import streamlit as st
import numpy as np
import plotly.express as px
import openai
import shap
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from fairlearn.reductions import ExponentiatedGradient, DemographicParity
from fairlearn.metrics import demographic_parity_difference
from PIL import Image
import matplotlib.pyplot as plt
import seaborn as sns

# === SETUP CONFIGURATION ===
st.set_page_config(page_title="AI Bias Analyzer", layout="wide")

# === LOAD MODEL & TOKENIZER ===
MODEL_NAME = "microsoft/deberta-v3-base"

try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)
except Exception as e:
    st.error(f"‚ö†Ô∏è Error loading AI model: {e}")
    st.stop()

# === SHAP EXPLAINER FIX: USING A TEXT MASKER ===
try:
    masker = shap.maskers.Text(tokenizer)
    explainer = shap.Explainer(model, masker=masker)
except Exception as e:
    st.error(f"‚ö†Ô∏è Error initializing SHAP explainer: {e}")
    st.stop()

# === LOAD DATA ===
uploaded_file = st.file_uploader("Upload a dataset", type=["csv"])

if uploaded_file:
    import pandas as pd

    df = pd.read_csv(uploaded_file)
    st.write("üìä Dataset Preview:", df.head())

    # Select columns
    target_col = st.selectbox("Select target column", df.columns)
    feature_cols = st.multiselect("Select feature columns", df.columns, default=df.columns.tolist())

    # === BIAS ANALYSIS ===
    with st.expander("‚öñÔ∏è Bias Analysis"):
        sensitive_attr = st.selectbox("Select sensitive attribute", df.columns)
        parity_diff = demographic_parity_difference(df[target_col], sensitive_features=df[sensitive_attr])
        st.write(f"üìâ Demographic Parity Difference: {parity_diff:.4f}")

    # === MODEL PREDICTIONS ===
    if st.button("Run Predictions"):
        inputs = tokenizer(df[feature_cols].astype(str).values.tolist(), padding=True, truncation=True, return_tensors="pt")
        with torch.no_grad():
            predictions = model(**inputs).logits.argmax(dim=-1).numpy()
        df["Prediction"] = predictions
        st.write("‚úÖ Predictions Completed!")

        # Explain Predictions
        with st.expander("üîç SHAP Explanations"):
            shap_values = explainer(df[feature_cols].astype(str).values.tolist())
            st.pyplot(shap.plots.text(shap_values))

    # === BIAS MITIGATION METHODS ===
    with st.expander("‚öñÔ∏è Bias Reduction Methods"):
        st.write("Explore bias reduction techniques:")
        st.markdown("- **Pre-processing**: Data balancing, re-weighting sensitive attributes")
        st.markdown("- **In-processing**: Adversarial debiasing, fairness constraints")
        st.markdown("- **Post-processing**: Calibration, fairness-aware thresholding")

        selected_method = st.selectbox("Select a bias mitigation method", ["None", "Reweighting", "Adversarial Training"])
        if selected_method != "None":
            st.write(f"Applying {selected_method}... üöÄ")
            # Implement methods here...

# === UX DESIGN EXPLANATION ===
with st.expander("üé® UX Design & Model Transparency"):
    st.write("This application follows transparency principles by providing clear explanations for model predictions.")
    st.image("ux_design.png", caption="UX Flowchart", use_column_width=True)

st.success("üéâ AI Bias Analyzer Ready!")