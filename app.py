# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rRSegdTULlczjwu11IQzYDFAIJmlE8TF
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import openai
import shap
import torch
from transformers import pipeline
from PIL import Image
import matplotlib.pyplot as plt
import seaborn as sns
from fairlearn.reductions import ExponentiatedGradient, DemographicParity
from fairlearn.metrics import demographic_parity_difference
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("microsoft/deberta-v3-base", ignore_mismatched_sizes=True)

tokenizer = AutoTokenizer.from_pretrained("microsoft/deberta-v3-base", use_fast=False)


# === SETUP CONFIGURATION ===
st.set_page_config(
    page_title="üêç Snakelets - AI Bias Analyzer",
    page_icon="üêç",
    layout="wide"
)

# === DEFINE COLORS & BRANDING ===
PRIMARY_COLOR = "#2E86C1"
SECONDARY_COLOR = "#F39C12"
HIGHLIGHT_COLOR = "#32aece"

# === HEADER ===
try:
    st.image("snake_logo.png", width=120)
except FileNotFoundError:
    st.warning("‚ö†Ô∏è Logo image 'snake_logo.png' not found. Please ensure it is in the project directory.")

st.markdown(
    f"<h1 style='color:{PRIMARY_COLOR}; text-align:center;'>üêç Snakelets AI Bias Analyzer</h1>",
    unsafe_allow_html=True
)
st.markdown(
    "<p style='text-align:center;'>Using AI to detect, explain, and mitigate biases</p>",
    unsafe_allow_html=True
)

# === FILE UPLOAD ===
st.subheader("üìÇ Upload a Dataset for Bias Analysis")
uploaded_file = st.file_uploader("Upload your dataset (CSV format)", type=["csv"])

if uploaded_file:
    df = pd.read_csv(uploaded_file)
    st.write("### üìä Data Preview:")
    st.dataframe(df)

    # === AI Bias Detection Model (Microsoft/HuggingFace) ===
    st.subheader("ü§ñ AI Bias Detection")
    model_name = "microsoft/deberta-v3-base"
    try:
        bias_model = pipeline("text-classification", model=model_name, device=0 if torch.cuda.is_available() else -1)
    except Exception as e:
        st.error(f"‚ö†Ô∏è Error loading AI model: {e}")

    # === User Selection for Bias Analysis ===
    target_column = st.selectbox("Select Target Column for Analysis:", df.columns)

    if st.button("üîç Analyze Bias"):
        st.write("üöÄ Running Bias Analysis...")

        # Compute model predictions
        predictions = bias_model(df[target_column].astype(str).tolist())

        # Convert predictions to DataFrame
        results_df = pd.DataFrame(predictions)
        results_df["Original Data"] = df[target_column].values

        # Display results
        st.write("### Bias Detection Results")
        st.dataframe(results_df)

        # === Bias Visualization ===
        st.subheader("üìà Bias Visualization")

        # Count occurrences of each label
        label_counts = results_df["label"].value_counts()
        fig = px.bar(label_counts, x=label_counts.index, y=label_counts.values, title="Bias Distribution")
        st.plotly_chart(fig)

        # === Explainability Using SHAP ===
        st.subheader("üßê Understanding Bias - SHAP Analysis")

        explainer = shap.Explainer(lambda x: bias_model([str(i) for i in x]))
        shap_values = explainer(df[target_column].astype(str))

        fig, ax = plt.subplots(figsize=(10, 6))
        shap.summary_plot(shap_values, df[target_column].astype(str), show=False)
        st.pyplot(fig)

    # === Bias Reduction Techniques ===
    st.subheader("‚öñÔ∏è Bias Reduction Methods")

    st.write("""
    Bias in machine learning models can be reduced using various techniques, each with **trade-offs**.
    Below are different bias mitigation strategies that you can test on your dataset.
    """)

    bias_methods = {
        "Reweighting": "Assign different weights to training samples based on underrepresented groups.",
        "Fair Representations": "Learn a new representation of the data that removes sensitive attribute correlations.",
        "Adversarial Debiasing": "Train a model with an adversary that learns to detect bias, forcing the main model to be unbiased.",
        "Post-processing (Threshold Adjustment)": "Adjust prediction thresholds to enforce fairness constraints.",
        "Fair Constraints (Demographic Parity)": "Apply constraints during training to enforce fairness across groups."
    }

    method_choice = st.selectbox("Choose a Bias Reduction Method", list(bias_methods.keys()))

    st.write(f"### üõ†Ô∏è {method_choice}")
    st.write(f"**How it works:** {bias_methods[method_choice]}")

    if method_choice == "Reweighting":
        st.write("This method re-weights the samples to balance the dataset.")
        # Example implementation (pseudo)
        df["weights"] = np.random.rand(len(df))  # Simulating new weights
        st.dataframe(df.head(5))

    elif method_choice == "Fair Representations":
        st.write("Transforming data to remove correlations with protected attributes.")
        # Example: Simulated transformed dataset
        transformed_df = df.copy()
        transformed_df[target_column] = np.random.permutation(transformed_df[target_column].values)
        st.dataframe(transformed_df.head(5))

    elif method_choice == "Adversarial Debiasing":
        st.write("Training an adversarial network to remove bias during training.")
        # Example: Simulated adversarial loss minimization
        adversarial_loss = np.random.rand()
        st.write(f"üîµ Adversarial Loss: {adversarial_loss:.4f}")

    elif method_choice == "Post-processing (Threshold Adjustment)":
        st.write("Adjusting decision thresholds for fairness.")
        threshold = st.slider("Choose Threshold Adjustment", 0.1, 1.0, 0.5)
        st.write(f"üìä Adjusted Threshold: {threshold}")

    elif method_choice == "Fair Constraints (Demographic Parity)":
        st.write("Applying demographic parity constraints.")
        try:
            constraints = DemographicParity()
            exp_grad = ExponentiatedGradient(df[target_column], constraints)
            model_fair = exp_grad.fit()
            st.write("‚úÖ Fair Model Trained with Constraints")
        except Exception as e:
            st.error(f"‚ö†Ô∏è Fair model training failed: {e}")

    # === Show Trade-offs ===
    st.subheader("‚öñÔ∏è Trade-offs of Bias Reduction Methods")
    st.write("""
    Each bias mitigation method has **advantages and drawbacks**:
    - **Reweighting**: ‚úÖ Easy to implement, ‚ùå May overfit to small groups.
    - **Fair Representations**: ‚úÖ Works well with deep learning, ‚ùå May reduce overall accuracy.
    - **Adversarial Debiasing**: ‚úÖ Strong performance, ‚ùå Requires more computation.
    - **Threshold Adjustment**: ‚úÖ Simple, ‚ùå May require manual tuning.
    - **Fair Constraints**: ‚úÖ Enforces strict fairness, ‚ùå Can limit predictive power.
    """)

    # === REFRESH BUTTON ===
    if st.button("üîÑ Refresh App"):
        st.cache_data.clear()
        st.rerun()

# === FOOTER ===
st.markdown(
    "<br><hr><p style='text-align:center;'>üêç Snakelets | AI Bias Analyzer | Made with ‚ù§Ô∏è</p>",
    unsafe_allow_html=True
)