# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rRSegdTULlczjwu11IQzYDFAIJmlE8TF
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import openai
import shap
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from fairlearn.reductions import ExponentiatedGradient, DemographicParity
from fairlearn.metrics import demographic_parity_difference
from PIL import Image
import matplotlib.pyplot as plt
import seaborn as sns

# === SETUP CONFIGURATION ===
st.set_page_config(page_title="AI Bias Analyzer", layout="wide")

# === LOAD PRETRAINED MODEL ===
MODEL_NAME = "microsoft/deberta-v3-base"

try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)
except Exception as e:
    st.error(f"ğŸš¨ Error loading AI model: {e}")

# === SHAP EXPLAINER SETUP ===
try:
    masker = shap.maskers.Text(tokenizer)
    explainer = shap.Explainer(model, masker=masker)
except Exception as e:
    st.warning(f"âš ï¸ SHAP explainer initialization error: {e}")

# === LOAD DATA ===
uploaded_file = st.file_uploader("ğŸ“‚ Upload a dataset (CSV)", type=["csv"])
if uploaded_file:
    try:
        df = pd.read_csv(uploaded_file)
        st.write("ğŸ“Š **Preview of dataset:**")
        st.dataframe(df.head())
    except Exception as e:
        st.error(f"ğŸš¨ Error loading file: {e}")

# === MODEL BIAS ANALYSIS ===
st.header("ğŸ” Bias Analysis")

if uploaded_file:
    target_column = st.selectbox("ğŸ¯ Select target variable:", df.columns)
    sensitive_attr = st.selectbox("ğŸ›‘ Select sensitive attribute:", df.columns)

    if target_column and sensitive_attr:
        try:
            demographic_parity = demographic_parity_difference(
                y_true=df[target_column],
                y_pred=df[target_column],  # Adjust as per model predictions
                sensitive_features=df[sensitive_attr]
            )
            st.write(f"ğŸ“¢ **Demographic Parity Difference:** {demographic_parity:.4f}")
        except Exception as e:
            st.error(f"ğŸš¨ Error computing fairness metric: {e}")

    # === SHAP Interpretation ===
    st.subheader("ğŸ” Explainability using SHAP")
    sample_text = st.text_area("ğŸ“ Enter a sample text for analysis:")

    if sample_text:
        try:
            tokens = tokenizer(sample_text, return_tensors="pt")
            shap_values = explainer(tokens)
            st.write("ğŸ“Š **SHAP Explanation for Prediction:**")
            shap.plots.text(shap_values)
        except Exception as e:
            st.error(f"ğŸš¨ Error generating SHAP values: {e}")

# === BIAS REDUCTION METHODS ===
st.header("âš–ï¸ Bias Reduction Techniques")

bias_methods = {
    "Reweighting": "Adjust sample weights to ensure fairness.",
    "Adversarial Debiasing": "Train an adversarial model to reduce bias.",
    "Post-Processing": "Modify predictions to achieve fairness.",
}

selected_method = st.selectbox("ğŸ“Œ Choose a bias reduction technique:", list(bias_methods.keys()))

st.write(f"ğŸ’¡ **Explanation:** {bias_methods[selected_method]}")

# === UX DESIGN EXPLANATION ===
st.header("ğŸ¨ UX Design & Model Transparency")

st.write(
    """
    - **Transparency**: The app provides fairness metrics and explanations.
    - **User Control**: Select bias reduction techniques and interpret model outputs.
    - **Visualizations**: Charts and SHAP plots help with decision-making.
    """
)

# === FINAL MESSAGE ===
st.success("ğŸ‰ AI Bias Analyzer Ready!")